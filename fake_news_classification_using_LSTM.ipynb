{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk. stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score \n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"clean_fake_real_data.pkl\", 'rb') as picklefile: \n",
    "    df = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>publication</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
       "      <td>Print They should pay all the back all the mon...</td>\n",
       "      <td>100percentfedup</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Re: Why Did Attorney General Loretta Lynch Ple...</td>\n",
       "      <td>Why Did Attorney General Loretta Lynch Plead T...</td>\n",
       "      <td>100percentfedup</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BREAKING: Weiner Cooperating With FBI On Hilla...</td>\n",
       "      <td>Red State : \\nFox News Sunday reported this mo...</td>\n",
       "      <td>100percentfedup</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...</td>\n",
       "      <td>Email Kayla Mueller was a prisoner and torture...</td>\n",
       "      <td>100percentfedup</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...</td>\n",
       "      <td>Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...</td>\n",
       "      <td>100percentfedup</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Muslims BUSTED: They Stole Millions In Gov’t B...   \n",
       "1  Re: Why Did Attorney General Loretta Lynch Ple...   \n",
       "2  BREAKING: Weiner Cooperating With FBI On Hilla...   \n",
       "3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...   \n",
       "4  FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...   \n",
       "\n",
       "                                             content      publication  type  \n",
       "0  Print They should pay all the back all the mon...  100percentfedup  fake  \n",
       "1  Why Did Attorney General Loretta Lynch Plead T...  100percentfedup  fake  \n",
       "2  Red State : \\nFox News Sunday reported this mo...  100percentfedup  fake  \n",
       "3  Email Kayla Mueller was a prisoner and torture...  100percentfedup  fake  \n",
       "4  Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...  100percentfedup  fake  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = WordNetLemmatizer()\n",
    "\n",
    "# text preprocessing\n",
    "corpus = []\n",
    "for i in range(0, len(df)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', str(df['content'][i])) # removing sepcial characters and numbers\n",
    "    review = review.lower() # lowering the text\n",
    "    review = review.split() \n",
    "    # removing stopwords and lemmatization\n",
    "    review = [ps.lemmatize(word) for word in review if not word in set(stopwords.words('english'))] \n",
    "    review = ' '.join(review)\n",
    "    df['content'][i] = review\n",
    "    corpus.append(review)\n",
    "\n",
    "all_words = []\n",
    "for text in corpus:\n",
    "    all_words.append(text.split())\n",
    "    \n",
    "words = [j for i in all_words for j in i]\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>publication</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
       "      <td>print pay back money plus interest entire fami...</td>\n",
       "      <td>100percentfedup</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Re: Why Did Attorney General Loretta Lynch Ple...</td>\n",
       "      <td>attorney general loretta lynch plead fifth bar...</td>\n",
       "      <td>100percentfedup</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BREAKING: Weiner Cooperating With FBI On Hilla...</td>\n",
       "      <td>red state fox news sunday reported morning ant...</td>\n",
       "      <td>100percentfedup</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...</td>\n",
       "      <td>email kayla mueller prisoner tortured isi chan...</td>\n",
       "      <td>100percentfedup</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...</td>\n",
       "      <td>email healthcare reform make america great sin...</td>\n",
       "      <td>100percentfedup</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Muslims BUSTED: They Stole Millions In Gov’t B...   \n",
       "1  Re: Why Did Attorney General Loretta Lynch Ple...   \n",
       "2  BREAKING: Weiner Cooperating With FBI On Hilla...   \n",
       "3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...   \n",
       "4  FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...   \n",
       "\n",
       "                                             content      publication  type  \n",
       "0  print pay back money plus interest entire fami...  100percentfedup  fake  \n",
       "1  attorney general loretta lynch plead fifth bar...  100percentfedup  fake  \n",
       "2  red state fox news sunday reported morning ant...  100percentfedup  fake  \n",
       "3  email kayla mueller prisoner tortured isi chan...  100percentfedup  fake  \n",
       "4  email healthcare reform make america great sin...  100percentfedup  fake  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(df['type']) # fake, real:two column one-hot encoding\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       ...,\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 155186 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each news.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "EMBEDDING_DIM = 100\n",
    "# tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(df['content'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (28665, 250)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(df['content'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of X:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y: (28665, 2)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(df['type']).values\n",
    "print('Shape of Y:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22932, 250) (22932, 2)\n",
      "(5733, 250) (5733, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 250, 100)          5000000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 5,117,506\n",
      "Trainable params: 5,117,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "# model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22932 samples, validate on 5733 samples\n",
      "Epoch 1/100\n",
      "22932/22932 [==============================] - 145s 6ms/step - loss: 0.3394 - accuracy: 0.8583 - val_loss: 0.2870 - val_accuracy: 0.9037\n",
      "Epoch 2/100\n",
      "22932/22932 [==============================] - 144s 6ms/step - loss: 0.0994 - accuracy: 0.9669 - val_loss: 0.2876 - val_accuracy: 0.9077\n",
      "Epoch 4/100\n",
      "22932/22932 [==============================] - 144s 6ms/step - loss: 0.0707 - accuracy: 0.9765 - val_loss: 0.3243 - val_accuracy: 0.8980\n",
      "Epoch 5/100\n",
      "22932/22932 [==============================] - 144s 6ms/step - loss: 0.0452 - accuracy: 0.9850 - val_loss: 0.3570 - val_accuracy: 0.9018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc2ee510e48>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=64, epochs=100, validation_data=(X_test, Y_test), callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-acc = 0.9954212454212454\n",
      "test-acc = 0.9017966160823304\n"
     ]
    }
   ],
   "source": [
    "train_pred = model.predict(X_train)\n",
    "test_pred = model.predict(X_test)\n",
    "print(\"train-acc = \" + str(accuracy_score(np.argmax(Y_train, axis=1), np.argmax(train_pred, axis=1))))\n",
    "print(\"test-acc = \" + str(accuracy_score(np.argmax(Y_test, axis=1), np.argmax(test_pred, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
